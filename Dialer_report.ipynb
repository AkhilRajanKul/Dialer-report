{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61542a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c78adeb",
   "metadata": {},
   "source": [
    "## Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427fd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27da5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Akhil\\Downloads\\project\\TradeX_raw\\STcon\"\n",
    "\n",
    "\n",
    "os.chdir(path)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "\n",
    "xlsx_files = [f for f in os.listdir(path) if f.endswith('.xlsx')]\n",
    "\n",
    "if len(xlsx_files) != 3:\n",
    "    raise ValueError(\"There should be exactly 3 .xlsx files in the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "st_1 = pd.read_excel(xlsx_files[0])\n",
    "st_2 = pd.read_excel(xlsx_files[1])\n",
    "st_3 = pd.read_excel(xlsx_files[2])\n",
    "\n",
    "\n",
    "concat_df = pd.concat([st_1,st_2,st_3], ignore_index=True)\n",
    "\n",
    "print(\"Length of st_1:\", len(st_1))\n",
    "print(\"Length of st_2:\", len(st_2))\n",
    "print(\"Length of st_3:\", len(st_3))\n",
    "print(\"Length of concatenated DataFrame:\", len(concat_df))\n",
    "\n",
    "# A = concat_df[concat_df['End of Call code'] != 'CAN_NOT_MAKE_CALL']\n",
    "\n",
    "output_path = r\"C:\\Users\\Akhil\\Downloads\\project\\TradeX_raw\\S_29_09.csv\"\n",
    "concat_df.to_csv(output_path, index=False)\n",
    "print(f\"CSV saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0eca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Akhil\\Downloads\\project\\TradeX_raw\"\n",
    "\n",
    "#\\pyton automation\\Tradex_dialer_raw\n",
    "# Change the working directory\n",
    "os.chdir(path)\n",
    "\n",
    "# Confirm the new working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9816b61",
   "metadata": {},
   "source": [
    "## load csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voiso = pd.read_csv('VT_12-16.csv',low_memory=False)\n",
    "tata = pd.read_csv('T_29_09.csv',low_memory=False)\n",
    "know = pd.read_csv('K_29_09.csv',low_memory=False)\n",
    "Qconn = pd.read_csv('Q_18_07.csv',low_memory=False)\n",
    "stringee = pd.read_csv('S_29_09.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a945682",
   "metadata": {},
   "source": [
    "## Check_point_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ae5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"voiso:\", voiso['Date and time'].unique()[:5])\n",
    "print(\"tata:\", tata['Call Start Date'].unique()[:5])\n",
    "print(\"know:\", know['Date and Time'].unique()[:5])\n",
    "print(\"Qconn:\", Qconn['Date time'].unique()[:5])\n",
    "print(\"stringee:\", stringee['Start time'].unique()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7716cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringee['Start time'] = pd.to_datetime(stringee['Start time']) + pd.Timedelta(hours=1, minutes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf857ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringee.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb57578",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringee['Answer duration'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert durations in hh:mm:ss format to timedelta\n",
    "def duration_to_timedelta(duration):\n",
    "    hours, minutes, seconds = map(int, duration.split(':'))\n",
    "    return timedelta(hours=hours, minutes=minutes, seconds=seconds)\n",
    "\n",
    "# Convert the 'Queue duration' and 'Answer duration' columns to timedelta\n",
    "stringee['Queue Duration (timedelta)'] = stringee['Queue duration'].apply(duration_to_timedelta)\n",
    "stringee['Answer Duration (timedelta)'] = stringee['Answer duration'].apply(duration_to_timedelta)\n",
    "\n",
    "# Calculate the total duration as timedelta\n",
    "stringee['Total Duration (timedelta)'] = stringee['Queue Duration (timedelta)'] + stringee['Answer Duration (timedelta)']\n",
    "\n",
    "# Convert the total duration back to hh:mm:ss format (remove \"0 days\")\n",
    "stringee['Total Duration'] = stringee['Total Duration (timedelta)'].apply(lambda x: str(x).split(\", \")[-1])\n",
    "\n",
    "# Drop intermediate timedelta columns if not needed\n",
    "stringee = stringee.drop(columns=['Queue Duration (timedelta)', 'Answer Duration (timedelta)', 'Total Duration (timedelta)'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68366f6",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tata = tata[['Call Start Date', 'Connected to Agent','Call Status','Answer Duration (HH:MM:SS)',\n",
    "                 'Hold Duration (HH:MM:SS)','Total Call Duration (HH:MM:SS)','Call Start Time','Customer Number']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a14a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_Know = know[['Date and Time', 'Agent Name','Call Status', 'Talk Time (hh:mm:ss)', 'Hold Time (hh:mm:ss)','Total Call Duration (hh:mm:ss)','Customer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_voiso =  voiso[['Date and time','Agent(s)','Disposition','Talk time','Duration','DNIS/To']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ef60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_qconn = Qconn[['Date time','Agent Mobile','Call Event','Transfer Duration','Duration','User Mobile']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc37b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_string = stringee[['Start time','Account','Call status','Answer duration','Hold duration','Total Duration','Customer number']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648efd97",
   "metadata": {},
   "source": [
    "## Voiso date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_voiso['Date and time'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eeaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to convert 'Date and time' from mm/dd/yyyy HH:mm:ss to yyyy/mm/dd HH:mm:ss\n",
    "# def fix_voiso_datetime(df, col_name):\n",
    "#     # Convert using pd.to_datetime, specify the format as mm/dd/yyyy\n",
    "#     df[col_name] = pd.to_datetime(df[col_name], errors='coerce', format='%m/%d/%Y %H:%M:%S') #\n",
    "    \n",
    "#     # Format the datetime to 'yyyy/mm/dd HH:mm:ss'\n",
    "#     df[col_name] = df[col_name].dt.strftime('%Y/%m/%d %H:%M:%S')\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Apply the function to 'Date and time' column in Voiso\n",
    "# new_voiso = fix_voiso_datetime(new_voiso, 'Date and time')\n",
    "\n",
    "# # Verify the result\n",
    "# print(\"Voiso Date and time after fixing format:\", new_voiso['Date and time'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60706a07",
   "metadata": {},
   "source": [
    "## string date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####check\n",
    "new_string['Start time'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fix 'new_string' Date Time format\n",
    "def fix_string_datetime(date_str):\n",
    "    try:\n",
    "        # Parse the date and time, assuming format mm/dd/yyyy hh:mm:ss AM/PM\n",
    "        parsed_date = pd.to_datetime(date_str, errors='coerce', format='%Y-%m-%d %H:%M:%S.%f')\n",
    "        return parsed_date\n",
    "    except Exception as e:\n",
    "        return pd.NaT  # Return NaT for any unparseable date\n",
    "\n",
    "# Apply this function to the 'Date and Time' column in 'new_string'\n",
    "new_string['Start time'] = new_string['Start time'].apply(fix_string_datetime)\n",
    "\n",
    "# Now format it to yyyy/mm/dd HH:mm:ss\n",
    "new_string['Start time'] = new_string['Start time'].dt.strftime('%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "# Check the result\n",
    "print(\"String unique date formats after fixing:\", new_string['Start time'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6682e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Know['Date and Time'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c022df",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_qconn['Date time'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date and Time' column in new_Know to datetime format\n",
    "new_qconn['Date time'] = pd.to_datetime(new_qconn['Date time'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "new_Know['Date and Time'] = pd.to_datetime(new_Know['Date and Time'], errors='coerce', format='%Y-%m-%d %H:%M:%S') #%d/%m/%Y\n",
    "\n",
    "# Now check the type of 'Date' again in both dataframes\n",
    "print(\"Data type of 'Date' in new_qconn after conversion:\", new_qconn['Date time'].dtype)\n",
    "print(\"Data type of 'Date' in new_Know after conversion:\", new_Know['Date and Time'].dtype)\n",
    "\n",
    "# Extract date and call start time for new_Know\n",
    "new_Know['Date'] = new_Know['Date and Time'].dt.date  # Extract date part\n",
    "new_Know['Call Start Time'] = new_Know['Date and Time'].dt.strftime('%H:%M:%S')  # Extract time part\n",
    "\n",
    "# Extract date and call start time for new_qconn (if you haven't done this yet)\n",
    "new_qconn['Date'] = new_qconn['Date time'].dt.date  # Extract date part\n",
    "new_qconn['Call Start Time'] = new_qconn['Date time'].dt.strftime('%H:%M:%S')  # Extract time part\n",
    "\n",
    "# Check the resulting dataframes to ensure extraction is successful\n",
    "print(new_qconn[['Date', 'Call Start Time']].head())\n",
    "print(new_Know[['Date', 'Call Start Time']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d571a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows where 'Date time' is NaT\n",
    "invalid_dates = new_qconn[new_qconn['Date time'].isna()]\n",
    "print(invalid_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_qconn['Date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type of the 'Date' column\n",
    "print(\"Data type of 'Date' in new_qconn:\", new_qconn['Date time'].dtype)\n",
    "print(\"Data type of 'Date' in new_Know:\", new_Know['Date and Time'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to separate Date and Call Start Time\n",
    "def split_date_time(df, col_name):\n",
    "    # Convert to datetime first (if not already done)\n",
    "    df[col_name] = pd.to_datetime(df[col_name], errors='coerce', format='%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "    # Extract the Date (yyyy/mm/dd) and Time (hh:mm:ss)\n",
    "    df['Date'] = df[col_name].dt.date\n",
    "    df['Call Start Time'] = df[col_name].dt.strftime('%H:%M:%S')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to each dataframe\n",
    "# new_voiso = split_date_time(new_voiso, 'Date and time')\n",
    "new_string = split_date_time(new_string, 'Start time')\n",
    "\n",
    "# Verify the changes\n",
    "print(new_qconn[['Date', 'Call Start Time']].head())\n",
    "# print(new_voiso[['Date', 'Call Start Time']].head())\n",
    "print(new_Know[['Date', 'Call Start Time']].head())\n",
    "print(new_string[['Date', 'Call Start Time']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3073ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_qconn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352866d",
   "metadata": {},
   "source": [
    "### copy of main dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bee70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tata_copy = new_tata.copy()\n",
    "know_copy = new_Know.copy()\n",
    "# voiso_copy = new_voiso.copy()\n",
    "qconn_copy = new_qconn.copy()\n",
    "string_copy = new_string.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c2dca5",
   "metadata": {},
   "source": [
    "### insert source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d660bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tata_copy['Source'] = 'Tata'\n",
    "know_copy['Source'] = 'Knowlarity'\n",
    "# voiso_copy['Source'] = 'Voiso'\n",
    "qconn_copy['Source'] = 'Qkonnect'\n",
    "string_copy['Source'] = 'Stringee'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a9745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tata_copy.dtypes)\n",
    "print(string_copy['Date'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b9080",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6928e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tata_copy.rename(columns={\n",
    "    'Call Start Date': 'Date',\n",
    "    'Connected to Agent': 'Dialer Name',\n",
    "    'Customer Number' : 'Number',\n",
    "    'Call Status': 'Call Status',\n",
    "    'Answer Duration (HH:MM:SS)': 'Talk Time',\n",
    "    'Hold Duration (HH:MM:SS)': 'Hold Time',\n",
    "    'Total Call Duration (HH:MM:SS)': 'Total Call Duration',\n",
    "    'Call Start Time': 'Call Start Time'\n",
    "}, inplace=True)\n",
    "\n",
    "know_copy.rename(columns={\n",
    "    'Date': 'Date',\n",
    "    'Agent Name': 'Dialer Name',\n",
    "    'Customer': 'Number',\n",
    "    'Call Status': 'Call Status',\n",
    "    'Talk Time (hh:mm:ss)': 'Talk Time',\n",
    "    'Hold Time (hh:mm:ss)': 'Hold Time',\n",
    "    'Total Call Duration (hh:mm:ss)': 'Total Call Duration',\n",
    "    'Call Start Time': 'Call Start Time'\n",
    "}, inplace=True)\n",
    "\n",
    "# voiso_copy.rename(columns={\n",
    "#     'Date': 'Date',\n",
    "#     'Agent(s)': 'Dialer Name',\n",
    "#     'DNIS/To': 'Number',\n",
    "#     'Disposition': 'Call Status',\n",
    "#     'Talk time': 'Talk Time',\n",
    "#     'Duration': 'Total Call Duration',\n",
    "#     'Call Start Time': 'Call Start Time'\n",
    "# }, inplace=True)\n",
    "\n",
    "qconn_copy.rename(columns={\n",
    "    'Date': 'Date',\n",
    "    'Agent Mobile': 'Dialer Name',\n",
    "    'User Mobile': 'Number',\n",
    "    'Call Event': 'Call Status',\n",
    "    'Transfer Duration': 'Talk Time',\n",
    "    'Duration': 'Total Call Duration',\n",
    "    'Call Start Time': 'Call Start Time'\n",
    "}, inplace=True)\n",
    "\n",
    "string_copy.rename(columns={\n",
    "    'Date': 'Date',    \n",
    "    'Account': 'Dialer Name',\n",
    "    'Customer number': 'Number',\n",
    "    'Call status': 'Call Status',\n",
    "    'Answer duration': 'Talk Time',\n",
    "    'Hold duration': 'Hold Time',\n",
    "    'Total Duration': 'Total Call Duration',\n",
    "    'Call Start Time': 'Call Start Time'\n",
    "}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ece389",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qconn_copy.shape)\n",
    "# print(voiso_copy.shape)\n",
    "print(know_copy.shape)  \n",
    "print(tata_copy.shape)\n",
    "print(string_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string_copy['Date'].unique())\n",
    "print(tata_copy['Date'].unique())\n",
    "print(know_copy['Date'].unique())\n",
    "# print(voiso_copy['Date'].unique())\n",
    "print(qconn_copy['Date'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3324e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tata_copy.shape, know_copy.shape, qconn_copy.shape,string_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select only the required columns from each dataframe\n",
    "tata_selected = tata_copy[[ 'Source','Date', 'Dialer Name','Number', 'Call Status','Call Start Time','Total Call Duration', 'Talk Time', 'Hold Time']]\n",
    "know_selected = know_copy[[ 'Source','Date', 'Dialer Name','Number' ,'Call Status', 'Call Start Time','Total Call Duration','Talk Time', 'Hold Time']]\n",
    "# voiso_selected = voiso_copy[[ 'Source','Date', 'Dialer Name', 'Number','Call Status','Call Start Time','Total Call Duration', 'Talk Time']]\n",
    "qconn_selected = qconn_copy[['Source','Date', 'Dialer Name','Number', 'Call Status','Call Start Time','Total Call Duration', 'Talk Time']]\n",
    "string_selected = string_copy[['Source','Date', 'Dialer Name','Number', 'Call Status','Call Start Time','Total Call Duration', 'Talk Time', 'Hold Time']]\n",
    "\n",
    "\n",
    "# Now concatenate\n",
    "combined = pd.concat([tata_selected, know_selected, qconn_selected, string_selected], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81591b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined.isnull().sum())\n",
    "print(combined.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053cf33d",
   "metadata": {},
   "source": [
    "## Check_point_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where Date is null\n",
    "null_date_entries = combined[combined['Date'].isnull()]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "null_date_entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54539c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df =combined.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine the regex to only remove unwanted patterns\n",
    "combined_df['Dialer Name'] = combined_df['Dialer Name'].str.replace(r\"\\s*\\([^)]*\\)|@.*|;.*\", \"\", regex=True)\n",
    "\n",
    "# Fill missing 'Dialer Name' values with their original values if they were numeric\n",
    "combined_df['Dialer Name'] = combined_df['Dialer Name'].fillna(combined['Dialer Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704215c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN in 'Hold time' with '00:00:00'\n",
    "combined_df['Hold Time'] = combined_df['Hold Time'].fillna('00:00:00')\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = combined_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fffe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'Dialer Name' is null\n",
    "A1 = A[A['Dialer Name'].notnull()]\n",
    "\n",
    "# Verify the result\n",
    "print(f\"Number of rows after removing null 'Dialer Name': {len(A1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in the 'Dialer Name' column\n",
    "null_dialer_name_count = combined_df['Dialer Name'].isnull().sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of null values in 'Dialer Name': {null_dialer_name_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d1957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9182c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3473a9",
   "metadata": {},
   "source": [
    "## Check_point_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c7a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dates_per_source = A1.groupby('Source')['Date'].unique()\n",
    "print(unique_dates_per_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_talk_time(talk_time):\n",
    "    # If the value is in seconds (only digits), convert it to hh:mm:ss\n",
    "    if re.match(r\"^\\d+$\", str(talk_time)):\n",
    "        seconds = int(talk_time)\n",
    "        hours = seconds // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        seconds = seconds % 60\n",
    "        return f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "    elif re.match(r\"^\\d+:\\d+:\\d+$\", str(talk_time)):\n",
    "        parts = talk_time.split(\":\")\n",
    "        hours = int(parts[0])\n",
    "        minutes = int(parts[1])\n",
    "        seconds = int(parts[2])\n",
    "        return f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "    else:\n",
    "        # Return the value as-is if it doesn't match expected formats\n",
    "        return talk_time\n",
    "\n",
    "# Apply the normalization function to the 'Talk Time' column\n",
    "A1['Talk Time'] = A1['Talk Time'].apply(normalize_talk_time)\n",
    "\n",
    "# Apply the normalization function to the 'Total Call Duration' column\n",
    "A1['Total Call Duration'] = A1['Total Call Duration'].apply(normalize_talk_time)\n",
    "\n",
    "\n",
    "print(A1[['Talk Time', 'Total Call Duration']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1118914",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_talk_time_formats = A1['Talk Time'].unique()\n",
    "\n",
    "print(f\"Unique formats in 'Talk Time': {unique_talk_time_formats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea1d25",
   "metadata": {},
   "source": [
    "#### Unique call status (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ba94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1['Call Status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1['Call Status'] = A1['Call Status'].apply(\n",
    "    lambda x: 'connected' if str(x).lower() == 'answered' else 'not connected'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71af8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(A1['Dialer Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc7129",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1['Dialer Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique 'Dialer Name' and sort from highest to lowest\n",
    "dialer_name_counts = A1['Dialer Name'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "print(dialer_name_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d35f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_df_1 = A1[~A1['Dialer Name'].isin([None, '---'])]\n",
    "\n",
    "combined_df_1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "combined_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c45685",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75531dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_d = pd.read_csv('Team_tradex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4dc2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_d.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505bc017",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref= ref_d.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = ref[~ref['Email'].str.contains('inactive', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4dde97",
   "metadata": {},
   "source": [
    "## Checkpoint_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe76d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [ref, combined_df_1]:\n",
    "    df['Dialer Name'] = (\n",
    "        df['Dialer Name']\n",
    "        .astype(str)  # Convert all values to strings\n",
    "        .str.replace(r'\\s+', ' ', regex=True)  # Replace multiple spaces with a single space\n",
    "        .str.strip()  # Remove leading and trailing spaces\n",
    "        .str.replace(r'@.*', '', regex=True)  # Remove everything from and after '@'\n",
    "        .str.replace(r'\\(.*', '', regex=True)  # Remove everything from and after '('\n",
    "        .str.strip()  # Remove any trailing spaces left after replacements\n",
    "        .str.lower()  # Convert all text to lowercase\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e78f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45631fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffc077",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = ref[ref.duplicated(subset=['Dialer Name','Email','Dialer'], keep=False)]\n",
    "dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = ref.drop_duplicates(subset=['Dialer Name','Email','Dialer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.rename(columns={'Email': 'CRM ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289285cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3633ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Dialer Name' in both dataframes is treated as a string\n",
    "combined_df_1['Dialer Name'] = combined_df_1['Dialer Name'].astype(str)\n",
    "ref['Dialer Name'] = ref['Dialer Name'].astype(str)\n",
    "\n",
    "# Merge the dataframes on 'Dialer Name'\n",
    "combined_df_2 = combined_df_1.merge(ref, how='left', left_on='Dialer Name', right_on='Dialer Name')\n",
    "\n",
    "\n",
    "combined_df_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ccd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'CRM ID' is null\n",
    "crm_id_null_df = combined_df_2[combined_df_2['CRM ID'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "crm_id_null_df['Dialer Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_id_null_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357970fa",
   "metadata": {},
   "source": [
    "## check_point_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80bf17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = crm_id_null_df.groupby('Source')['Dialer Name'].unique()\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm_id_null_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d061e",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd7eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7446b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dialers = combined_df_2[combined_df_2['CRM ID'].notnull() & combined_df_2['Talk Time'].notnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d90afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dialers.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dialers = Dialers.drop_duplicates(subset=['Number','Call Start Time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74776c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "XXX = combined_df_2[combined_df_2['Date'].isnull()]\n",
    "XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe3b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dialers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "Dialers['Date'] = pd.to_datetime(Dialers['Date'], format='%Y-%m-%d', errors='coerce')\n",
    "Dialers['Call Start Time'] = pd.to_datetime(\n",
    "    Dialers['Date'].dt.strftime('%Y-%m-%d').fillna('1900-01-01') + ' ' + Dialers['Call Start Time'],\n",
    "    format='%Y-%m-%d %H:%M:%S',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Convert durations\n",
    "Dialers['Total Call Duration'] = Dialers['Total Call Duration'].apply(\n",
    "    lambda x: pd.to_timedelta(x) if isinstance(x, str) else pd.Timedelta(0)\n",
    ")\n",
    "\n",
    "# Sort\n",
    "Dialers = Dialers.sort_values(by=['Date', 'CRM ID', 'Call Start Time']).reset_index(drop=True)\n",
    "\n",
    "# Init columns\n",
    "Dialers['Call Gap'] = 'No'\n",
    "Dialers['Gap Duration'] = '00:00:00'\n",
    "\n",
    "# Define working hours\n",
    "start_time = pd.to_datetime('09:30:00').time()\n",
    "end_time = pd.to_datetime('18:30:00').time()\n",
    "\n",
    "# Loop through to calculate gaps\n",
    "for i in range(1, len(Dialers)):\n",
    "    same_crm = Dialers.loc[i, 'CRM ID'] == Dialers.loc[i - 1, 'CRM ID']\n",
    "    same_date = Dialers.loc[i, 'Date'] == Dialers.loc[i - 1, 'Date']  \n",
    "\n",
    "    if same_crm and same_date:\n",
    "        current_time = Dialers.loc[i, 'Call Start Time'].time()\n",
    "        previous_time = Dialers.loc[i - 1, 'Call Start Time'].time()\n",
    "\n",
    "        previous_end = Dialers.loc[i - 1, 'Call Start Time'] + Dialers.loc[i - 1, 'Total Call Duration']\n",
    "        gap_duration = Dialers.loc[i, 'Call Start Time'] - previous_end\n",
    "\n",
    "        if gap_duration.total_seconds() < 0:\n",
    "            gap_duration = timedelta(0)\n",
    "\n",
    "        # Store formatted gap\n",
    "        total_seconds = int(gap_duration.total_seconds())\n",
    "        hours = total_seconds // 3600\n",
    "        minutes = (total_seconds % 3600) // 60\n",
    "        seconds = total_seconds % 60\n",
    "        Dialers.loc[i, 'Gap Duration'] = f\"{hours:02}:{minutes:02}:{seconds:02}\"\n",
    "\n",
    "        # Set 'Call Gap' only if within working hours\n",
    "        if start_time <= current_time <= end_time and start_time <= previous_time <= end_time:\n",
    "            Dialers.loc[i, 'Call Gap'] = 'Yes' if gap_duration > timedelta(minutes=1) else 'No'\n",
    "\n",
    "Dialers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd1c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dialers.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8fb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "invalid_values = []\n",
    "\n",
    "# Function to convert mixed formats to seconds\n",
    "def to_seconds(value):\n",
    "    try:\n",
    "        if isinstance(value, pd.Timedelta):\n",
    "            return int(value.total_seconds())  # Convert timedelta to seconds\n",
    "        elif re.match(r\"^\\d{1,2}:\\d{1,2}:\\d{1,2}$\", str(value)):\n",
    "            parts = list(map(int, value.split(':')))\n",
    "            while len(parts) < 3:\n",
    "                parts.insert(0, 0)  \n",
    "            return parts[0] * 3600 + parts[1] * 60 + parts[2]\n",
    "        elif str(value).isdigit():\n",
    "            return int(value)\n",
    "        else:\n",
    "            invalid_values.append(value)\n",
    "            return value\n",
    "    except Exception:\n",
    "        invalid_values.append(value)\n",
    "        return value\n",
    "\n",
    "# Replace missing values with empty strings\n",
    "Dialers['Talk Time'] = Dialers['Talk Time'].fillna('')\n",
    "Dialers['Hold Time'] = Dialers['Hold Time'].fillna('')\n",
    "Dialers['Total Call Duration'] = Dialers['Total Call Duration'].fillna('')\n",
    "Dialers['Gap Duration'] = Dialers['Gap Duration'].fillna('')\n",
    "\n",
    "# Convert Talk Time, Hold Time, and Total Call Duration to seconds\n",
    "Dialers['Talk Time (seconds)'] = Dialers['Talk Time'].apply(to_seconds)\n",
    "Dialers['Hold Time (seconds)'] = Dialers['Hold Time'].apply(to_seconds)\n",
    "Dialers['Total Duration (seconds)'] = Dialers['Total Call Duration'].apply(to_seconds)\n",
    "Dialers['Gap Duration (seconds)'] = Dialers['Gap Duration'].apply(to_seconds)\n",
    "\n",
    "# Notify invalid values\n",
    "if invalid_values:\n",
    "    print(\"Invalid values found in 'Talk Time', 'Total Call Duration', or 'Hold Time':, or 'Call Gap Duration':\")\n",
    "    print(invalid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2db2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff30cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dialers.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1539f2",
   "metadata": {},
   "source": [
    "### Change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Total_Duration'] = pd.to_timedelta(df['Total_Duration'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff46615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'CRM ID' and 'Date' and calculate \n",
    "A = Dialers.groupby(['CRM ID', 'Date']).agg(\n",
    "    Total_Dialed_Calls=('Call Status', 'count'),\n",
    "    Unique_Dialed_Numbers=('Number', 'nunique'),\n",
    "    Total_Connected_Calls=('Call Status', lambda x: (x == 'connected').sum()),\n",
    "    Total_Number_of_Call_Gap=('Call Gap', lambda x: (x == 'Yes').sum()),\n",
    "    Total_Call_GT_30=('Talk Time (seconds)', lambda x: ((Dialers.loc[x.index, 'Call Status'] == 'connected') & (x > 30)).sum()),\n",
    "    Total_Duration=('Total Duration (seconds)', 'sum'),\n",
    "    Total_Talk_Time=('Talk Time (seconds)', lambda x: x[Dialers.loc[x.index, 'Call Status'] == 'connected'].sum()),\n",
    "    Total_Talk_Time_GT_30=('Talk Time (seconds)', lambda x: x[(Dialers.loc[x.index, 'Call Status'] == 'connected') & (x > 30)].sum()),\n",
    "    Total_Connected_Hold_Time=('Hold Time (seconds)', lambda x: x[Dialers.loc[x.index, 'Call Status'] == 'connected'].sum()), \n",
    "    Total_Gap_Duration=('Gap Duration (seconds)', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Fix: Subtract 1hr only if greater, else keep the original value\n",
    "A['Total_Gap_Duration'] = A['Total_Gap_Duration'].apply(lambda x: x - 3600 if x > 3600 else x)\n",
    "\n",
    "\n",
    "A['Avg_Gap_per_call'] = A['Total_Gap_Duration'] / A['Total_Dialed_Calls']\n",
    "\n",
    "# New: Gap Duration After Leverage (give 45 seconds per call, subtract from actual gap used, add to 0 if negative)\n",
    "A['Gap Duration After Leverage'] = (A['Total_Gap_Duration'] - (A['Total_Dialed_Calls'] * 45)).clip(lower=0)\n",
    "\n",
    "# Recalculate Login Hours after updated gap\n",
    "A['Login Hours'] = A['Total_Duration'] + A['Total_Gap_Duration'] + 3600\n",
    "A['Login Hours'] = pd.to_timedelta(A['Login Hours'], unit='s')\n",
    "A['Login Hours'] = A['Login Hours'].apply(lambda x: str(x).split()[-1])\n",
    "\n",
    "# Convert 'Login Hours' string (hh:mm:ss) to timedelta\n",
    "A['Login Hours (Timedelta)'] = pd.to_timedelta(A['Login Hours'])\n",
    "\n",
    "# Defining attendance logic\n",
    "def mark_attendance(td):\n",
    "    if td < pd.Timedelta(hours=4, minutes=30):\n",
    "        return 'Absent'\n",
    "    elif td < pd.Timedelta(hours=6):\n",
    "        return 'Half Day'\n",
    "    elif td < pd.Timedelta(hours=8, minutes=30):\n",
    "        return 'Warning'\n",
    "    else:\n",
    "        return 'Present'\n",
    "\n",
    "\n",
    "A['Attendance'] = A['Login Hours (Timedelta)'].apply(mark_attendance)\n",
    "\n",
    "\n",
    "A.drop(columns='Login Hours (Timedelta)', inplace=True)\n",
    "\n",
    "\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AB = A['Attendance'].unique()\n",
    "AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07227728",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter unique CRM ID and select specific columns\n",
    "unique_crm_ref = ref.drop_duplicates(subset=['CRM ID'])[['CRM ID','Employee code', 'Full Name','Pool', 'TL','Vertical']]\n",
    "\n",
    "unique_crm_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e35f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique dates from DataFrame A\n",
    "all_dates = A['Date'].unique()\n",
    "\n",
    "# Create a DataFrame with all CRM IDs from unique_crm_ref and all dates\n",
    "date_crm_combinations = pd.MultiIndex.from_product(\n",
    "    [unique_crm_ref['CRM ID'], all_dates],\n",
    "    names=['CRM ID', 'Date']\n",
    ").to_frame(index=False)\n",
    "\n",
    "\n",
    "\n",
    "merged = date_crm_combinations.merge(\n",
    "    A,\n",
    "    how='left',\n",
    "    on=['CRM ID', 'Date']\n",
    ").fillna(0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer-merge with unique_crm_ref to retain all CRM IDs\n",
    "merged_df = unique_crm_ref.merge(\n",
    "    merged,\n",
    "    how='outer',\n",
    "    on='CRM ID'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c896d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN or missing values with 0\n",
    "columns_to_format = ['Total_Dialed_Calls', 'Unique_Dialed_Numbers','Total_Connected_Calls','Total_Number_of_Call_Gap', 'Total_Call_GT_30','Total_Duration','Total_Talk_Time','Total_Talk_Time_GT_30','Total_Connected_Hold_Time','Total_Gap_Duration']\n",
    "merged_df[columns_to_format] = merged_df[columns_to_format].fillna(0)\n",
    "\n",
    "# Convert specified columns to integers\n",
    "merged_df[columns_to_format] = merged_df[columns_to_format].astype(int)\n",
    "\n",
    "# Reorder columns\n",
    "formatted_df = merged_df[['Date', 'Pool', 'TL', 'CRM ID','Employee code', 'Full Name', 'Vertical'] + [col for col in merged_df.columns if col not in ['Date', 'Pool', 'TL', 'CRM ID','Employee code', 'Full Name', 'Vertical']]]\n",
    "\n",
    "formatted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a260d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846847ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_hhmmss(seconds):\n",
    "    if pd.isna(seconds):\n",
    "        return None  # or \"00:00:00\" if you prefer\n",
    "    total_seconds = round(seconds)\n",
    "    hours = total_seconds // 3600\n",
    "    remaining = total_seconds % 3600\n",
    "    minutes = remaining // 60\n",
    "    seconds = remaining % 60\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02}\"  # hh:mm:ss\n",
    "\n",
    "\n",
    "columns_to_convert = [\n",
    "    'Total_Duration', \n",
    "    'Total_Talk_Time', \n",
    "    'Total_Talk_Time_GT_30', \n",
    "    'Total_Connected_Hold_Time', \n",
    "    'Total_Gap_Duration',\n",
    "    'Avg_Gap_per_call',\n",
    "    'Gap Duration After Leverage'\n",
    "]\n",
    "\n",
    "for col in columns_to_convert:\n",
    "    try:\n",
    "        formatted_df[col] = formatted_df[col].apply(seconds_to_hhmmss)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in column: {col}\")\n",
    "        raise e  # re-raise the error so you still get the traceback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e11f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import timedelta\n",
    "\n",
    "# def seconds_to_hhmmss(seconds):\n",
    "#     total_seconds = round(seconds)\n",
    "#     hours = total_seconds // 3600\n",
    "#     remaining = total_seconds % 3600\n",
    "#     minutes = remaining // 60\n",
    "#     seconds = remaining % 60\n",
    "#     return f\"{hours:02}:{minutes:02}:{seconds:02}\"  # hh:mm:ss\n",
    "\n",
    "\n",
    "# columns_to_convert = [\n",
    "#     'Total_Duration', \n",
    "#     'Total_Talk_Time', \n",
    "#     'Total_Talk_Time_GT_30', \n",
    "#     'Total_Connected_Hold_Time', \n",
    "#     'Total_Gap_Duration',\n",
    "#     'Avg_Gap_per_call',\n",
    "#     'Gap Duration After Leverage'\n",
    "# ]\n",
    "\n",
    "# for col in columns_to_convert:\n",
    "#     formatted_df[col] = formatted_df[col].apply(seconds_to_hhmmss)\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f2a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df = formatted_df.sort_values(by=['Date','CRM ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a1b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9135f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df['Attendance'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a4646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "formatted_df['Login Hours'] = formatted_df['Login Hours'].apply(lambda x: '00:00:00' if x == 0 else x)\n",
    "\n",
    "formatted_df['Attendance'] = formatted_df['Attendance'].apply(lambda x: 'Absent' if x == 0 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fda151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Total Call Duration' to hh:mm:ss format\n",
    "Dialers[\"Total Call Duration\"] = Dialers[\"Total Call Duration\"].apply(lambda x: str(x).split(\" \")[-1])\n",
    "Dialers['Number'] = \"'\" + Dialers['Number'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Dialers.drop(columns=['Talk Time (seconds)','Hold Time (seconds)','Total Duration (seconds)','Gap Duration (seconds)','Dialer'])\n",
    "D['Number'] = D['Number'].astype(str).str.split('.').str[0]\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83384ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252fe75",
   "metadata": {},
   "source": [
    "# Transaction_stich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "import itables\n",
    "itables.init_notebook_mode(all_interactive=True)\n",
    "from itables import show\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = D.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c358f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_calls = df.drop_duplicates(subset=[\"Date\", \"CRM ID\", \"Number\",\"Call Status\"])[[\"Date\", \"CRM ID\", \"Number\",\"Call Status\",\"Pool\",\"TL\",\"Full Name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to clean phone numbers\n",
    "def clean_phone(phone):\n",
    "    if pd.isna(phone):\n",
    "        return \"\"\n",
    "    cleaned = ''.join(filter(str.isdigit, str(phone)))\n",
    "    return cleaned[-10:]\n",
    "\n",
    "\n",
    "unique_calls[\"Consider\"] = unique_calls[\"Number\"].apply(clean_phone)\n",
    "\n",
    "\n",
    "unique_calls[\"Len\"] = unique_calls[\"Consider\"].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef39933",
   "metadata": {},
   "source": [
    "# Trnx_Db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "import numpy as np\n",
    "import itables\n",
    "itables.init_notebook_mode(all_interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "D.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your connection parameters\n",
    "conn_params = {\n",
    "    'host': 'your_host',\n",
    "    'user': 'your_user',\n",
    "    'password': 'your_password',\n",
    "    'database': 'your_database'\n",
    "}\n",
    "\n",
    "\n",
    "D['Date'] = pd.to_datetime(D['Date'])\n",
    "dates_only = D['Date'].dt.date\n",
    "\n",
    "\n",
    "overall_start_date = datetime.combine(dates_only.min(), datetime.min.time())\n",
    " \n",
    "overall_end_date = datetime.combine(dates_only.max(), datetime.min.time()) + timedelta(days=1)\n",
    " \n",
    "print(f\"Data fetch range from {overall_start_date} to {overall_end_date}\")\n",
    " \n",
    "# Initialize empty dataframe and loaded_dates set if not already\n",
    "try:\n",
    "    partial_df\n",
    "except NameError:\n",
    "    partial_df = pd.DataFrame()\n",
    " \n",
    "try:\n",
    "    loaded_dates\n",
    "except NameError:\n",
    "    loaded_dates = set()\n",
    "\n",
    "\n",
    "# Function to fetch data chunk for a date range from MySQL\n",
    "def fetch_data_for_date_range(start_date, end_date):\n",
    "    query_template = \"\"\"\n",
    "    SELECT *\n",
    "    FROM manual_transaction_log\n",
    "    WHERE time >= %s AND time < %s\n",
    "      AND action NOT IN ('Delete', 'Update')\n",
    "      AND type NOT IN ('Pending', 'Withdraw Request', 'Deposit Request')\n",
    "      AND typeDetail NOT IN ('SLTP', 'Adjustment')\n",
    "    ORDER BY time\n",
    "    \"\"\"\n",
    "    conn = mysql.connector.connect(**conn_params)\n",
    "    df = pd.read_sql(query_template, conn, params=(start_date, end_date))\n",
    "    conn.close()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e6480",
   "metadata": {},
   "source": [
    "# VPN Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_start = overall_start_date\n",
    "if not loaded_dates:\n",
    "    print(\"Starting fresh from\", current_start.strftime('%Y-%m-%d'))\n",
    "else:\n",
    "    # Resume from last loaded date + 1 day\n",
    "    last_loaded = max(datetime.strptime(d, '%Y-%m-%d') for d in loaded_dates)\n",
    "    current_start = last_loaded + timedelta(days=1)\n",
    "    print(\"Resuming from\", current_start.strftime('%Y-%m-%d'))\n",
    "\n",
    "accumulated_rows = len(partial_df)\n",
    "\n",
    "while current_start < overall_end_date:\n",
    "    current_end = current_start + timedelta(days=1)\n",
    "    current_start_str = current_start.strftime('%Y-%m-%d')\n",
    "\n",
    "    if current_start_str in loaded_dates:\n",
    "        print(f\"Skipping already loaded date {current_start_str}\")\n",
    "        current_start = current_end\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"Loading data from {current_start_str} to {current_end.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        df_chunk = fetch_data_for_date_range(current_start, current_end)\n",
    "        num_rows = len(df_chunk)\n",
    "        accumulated_rows += num_rows\n",
    "        print(f\"Loaded {num_rows} rows; Total rows loaded so far: {accumulated_rows}\")\n",
    "\n",
    "        # Append chunk to partial_df (reset index for concatenation)\n",
    "        partial_df = pd.concat([partial_df, df_chunk], ignore_index=True)\n",
    "\n",
    "        loaded_dates.add(current_start_str)\n",
    "\n",
    "        current_start = current_end\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {current_start_str}: {e}\")\n",
    "        print(\"Stop and rerun this cell to resume.\")\n",
    "        break  # Stop the loop to fix issue or rerun later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98fadd",
   "metadata": {},
   "source": [
    "### final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b0257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = partial_df.copy()\n",
    "print(f\"Final combined data with {len(final_df)} rows is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887edad",
   "metadata": {},
   "source": [
    "## Consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99971f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = final_df.copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70374cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d268b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all rows where ticketId and time are duplicated\n",
    "duplicates = combined_df[combined_df.duplicated(subset=['ticketId', 'time','accountId'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d08f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many\n",
    "print(f\"Total duplicate rows: {len(duplicates)}\")\n",
    "\n",
    "# Show unique duplicate pairs (ticketId, time) with counts\n",
    "duplicate_summary = (\n",
    "    duplicates.groupby(['ticketId', 'time'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values(by='count', ascending=False)\n",
    ")\n",
    "\n",
    "print(duplicate_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f003325",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on specific subset\n",
    "final_df = combined_df.drop_duplicates(subset=['ticketId', 'time', 'accountId'], keep='first')\n",
    "\n",
    "print(f\"Rows before: {len(combined_df)}\")\n",
    "print(f\"Rows after dropping duplicates: {len(final_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4698a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = final_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28a453",
   "metadata": {},
   "source": [
    "### Min date and max date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e7ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dd/mm/yyyy H:MM\n",
    "parsed_ddmmyyyy = pd.to_datetime(check['time'], format=\"%d/%m/%Y %H:%M\", errors='coerce')\n",
    "\n",
    "# Wherever parsing succeeded, replace the original with the standardized format\n",
    "check.loc[parsed_ddmmyyyy.notna(), 'time'] = parsed_ddmmyyyy.dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Now ensure the whole column is in datetime\n",
    "check['time'] = pd.to_datetime(check['time'], errors='coerce')\n",
    "\n",
    "print(check['time'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35262c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "check['date_only'] = check['time'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266dc4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get min and max date\n",
    "min_date = check['date_only'].min()\n",
    "max_date = check['date_only'].max()\n",
    "print(f\"Min date: {min_date}, Max date: {max_date}\")\n",
    "\n",
    "# Create a date range for every day between min and max date\n",
    "full_date_range = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "# Prepare DataFrame for month-wise date counts and missing dates\n",
    "months = check['date_only'].apply(lambda x: x.replace(day=1)).unique()\n",
    "months.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4425b52",
   "metadata": {},
   "source": [
    "### month wise check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "# Convert min_date and max_date to Timestamp if they are date\n",
    "min_date_ts = pd.to_datetime(min_date)\n",
    "max_date_ts = pd.to_datetime(max_date)\n",
    "\n",
    "print(\"Month-wise breakdown and missing dates:\")\n",
    "\n",
    "for month_start in months:\n",
    "    month_start = pd.to_datetime(month_start)\n",
    "    month_end = month_start + MonthEnd(0)\n",
    "    # Dates existing in this month\n",
    "    dates_in_month = check[check['date_only'].between(month_start.date(), month_end.date())]['date_only'].unique()\n",
    "    dates_in_month = pd.to_datetime(dates_in_month)\n",
    "    \n",
    "    # Expected dates in month (from min to max, constrained by dataset range)\n",
    "    expected_start = max(month_start, min_date_ts)\n",
    "    expected_end = min(month_end, max_date_ts)\n",
    "    expected_dates = pd.date_range(\n",
    "        start=expected_start,\n",
    "        end=expected_end\n",
    "    )\n",
    "    \n",
    "    # Missing dates in this month\n",
    "    missing_dates = expected_dates.difference(dates_in_month)\n",
    "    \n",
    "    print(f\"{month_start.strftime('%B %Y')}:\")\n",
    "    print(f\"  Dates present: {len(dates_in_month)}\")\n",
    "    print(f\"  Dates missing: {len(missing_dates)}\")\n",
    "    if len(missing_dates) > 0:\n",
    "        missing_str = ', '.join(missing_dates.strftime('%Y-%m-%d'))\n",
    "        print(f\"  Missing dates: {missing_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a3def",
   "metadata": {},
   "source": [
    "### caluclation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_1 = check.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097266a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_1['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = full_df_1[\n",
    "    (~full_df_1['action'].isin(['Delete', 'Update'])) &\n",
    "    (~full_df_1['type'].isin(['Pending', 'Withdraw Request', 'Deposit Request'])) &\n",
    "    (~full_df_1['typeDetail'].isin(['SLTP', 'Adjustment']))\n",
    "]\n",
    "\n",
    "filtered_df['Date'] = pd.to_datetime(filtered_df['time']).dt.date\n",
    "grouped = filtered_df.groupby(['accountId', 'Date'])\n",
    "\n",
    "summary_df = grouped.apply(lambda g: pd.Series({\n",
    "    'deposit_count': (g['typeDetail'] == 'Deposit').sum(),\n",
    "    'deposit_amount': g.loc[g['typeDetail'] == 'Deposit', 'amount'].sum(),\n",
    "    'withdrawal_count': (g['typeDetail'] == 'Withdrawal').sum(),\n",
    "    'withdrawal_amount': g.loc[g['typeDetail'] == 'Withdrawal', 'amount'].sum(),\n",
    "    'last_activity': g['time'].max(),\n",
    "    'deposit_time': ''.join(\n",
    "        f'[{i.strftime(\"%Y-%m-%d %H:%M:%S\")}]' for i in sorted(g.loc[g['typeDetail'] == 'Deposit', 'time'])\n",
    "    ),\n",
    "    'deposit_distribution': ''.join(\n",
    "        f'[{amt}]' for _, amt in sorted(zip(g.loc[g['typeDetail'] == 'Deposit', 'time'], g.loc[g['typeDetail'] == 'Deposit', 'amount']))\n",
    "    )\n",
    "})).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = summary_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f31fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9cb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final['Date'] = pd.to_datetime(final['Date']).dt.date\n",
    "\n",
    "for col in ['deposit_amount', 'withdrawal_amount']:\n",
    "    final[col] = final[col].fillna(0).astype('int64')\n",
    "\n",
    "final['last_activity'] = final['last_activity'].dt.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e15d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdb = final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_update_till = cdb['Date'].max()\n",
    "last_update_till"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ffd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdb['Date'] = pd.to_datetime(cdb['Date'])\n",
    "unique_calls['Date'] = pd.to_datetime(unique_calls['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3e2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dates = unique_calls['Date'].dt.date.unique()\n",
    "filtered_cdb = cdb[cdb['Date'].dt.date.isin(unique_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702cdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21bf538",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# OAuth client details and refresh token\n",
    "client_id = '1000.XYZABCDEF1234567890'\n",
    "client_secret = 'your_client_secret_here'\n",
    "refresh_token = '1000.abcdef1234567890abcdef1234567890.abcdef1234567890abcdef1234567890'\n",
    "\n",
    "owner_email = 'ab@admin.com'\n",
    "workspace_name = 'Zoho ky Analytics'\n",
    "\n",
    "token_file = 'access_token.json'\n",
    "\n",
    "def save_token(token):\n",
    "    with open(token_file, 'w') as f:\n",
    "        json.dump({'access_token': token}, f)\n",
    "\n",
    "def load_token():\n",
    "    if os.path.exists(token_file):\n",
    "        with open(token_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return data.get('access_token')\n",
    "    return None\n",
    "\n",
    "def refresh_access_token():\n",
    "    url = \"https://accounts.zoho.com/oauth/v2/token\"\n",
    "    params = {\n",
    "        'refresh_token': refresh_token,\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret,\n",
    "        'grant_type': 'refresh_token'\n",
    "    }\n",
    "    response = requests.post(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        tokens = response.json()\n",
    "        new_token = tokens['access_token']\n",
    "        print(\"Refreshed Access Token:\", new_token)\n",
    "        save_token(new_token)\n",
    "        return new_token\n",
    "    else:\n",
    "        raise Exception(f\"Failed to refresh token: {response.text}\")\n",
    "\n",
    "def run_query(access_token, client_ids):\n",
    "    # Convert all IDs to string (no lowercasing)\n",
    "    client_ids_str = [str(x) for x in client_ids]\n",
    "    id_list_str = \"', '\".join(client_ids_str)\n",
    "    id_list_str = f\"'{id_list_str}'\"\n",
    "\n",
    "    sql_query = f\"\"\"\n",
    "        SELECT\n",
    "            c.\"Account Id\" as ID,\n",
    "            f.\"Phone\",\n",
    "            c.\"Phone\"\n",
    "        FROM \"Client\" c\n",
    "        LEFT JOIN \"Followup\" f ON f.\"Converted Contact\" = c.\"Id\"\n",
    "        WHERE c.\"Account Id\" IN ({id_list_str})\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"https://analyticsapi.zoho.com/api/{owner_email}/{workspace_name}\"\n",
    "    headers = {'Authorization': f\"Zoho-oauthtoken {access_token}\"}\n",
    "    payload = {\n",
    "        'ZOHO_ACTION': 'EXPORT',\n",
    "        'ZOHO_OUTPUT_FORMAT': 'JSON',\n",
    "        'ZOHO_SQLQUERY': sql_query,\n",
    "        'ZOHO_API_VERSION': '1.0'\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=payload)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Usage Example:\n",
    "client_ids = filtered_cdb['accountId'].unique().tolist()\n",
    "#client_ids = ['825132', '824819', '825118']  # Example; replace with actual unique IDs\n",
    "\n",
    "# Load token from storage or refresh if not present\n",
    "access_token = load_token()\n",
    "if not access_token:\n",
    "    access_token = refresh_access_token()\n",
    "\n",
    "response = run_query(access_token, client_ids)\n",
    "\n",
    "if response.status_code == 401:\n",
    "    print(\"Access token expired, refreshing...\")\n",
    "    access_token = refresh_access_token()\n",
    "    response = run_query(access_token, client_ids)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(\"Query Result:\", data)\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d8f1fa",
   "metadata": {},
   "source": [
    "# rows section above should not be null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data['response']['result']['column_order']\n",
    "rows = data['response']['result']['rows']\n",
    " \n",
    "client_phone_1 = pd.DataFrame(rows, columns=columns)\n",
    "client_phone_1.columns = ['Account Id', 'fphone','Phone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ea881",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_phone_1.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f42dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_phone_1 = client_phone_1.rename(columns={'Account Id' : \"accountId\", 'Phone' : 'f.Phone'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cecc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_phone_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_phone(phone):\n",
    "    if pd.isna(phone):\n",
    "        return \"\"\n",
    "    cleaned = ''.join(filter(str.isdigit, str(phone)))\n",
    "    return cleaned[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_phone_1['f.Phone'] = client_phone_1['f.Phone'].apply(clean_phone)\n",
    "client_phone_1['fphone'] = client_phone_1['fphone'].apply(clean_phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e25791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate the phone number based on the given conditions\n",
    "def validate_phone(row):\n",
    "    fphone = str(row['fphone'])\n",
    "    fphone_check = str(row['f.Phone'])\n",
    "\n",
    "    # Check if the fPhone is valid (i.e., it should have 10 digits)\n",
    "    if len(fphone_check) != 10 or fphone_check[0] not in ['6', '7', '8', '9']:\n",
    "        return fphone  # If invalid, return value from fphone column\n",
    "    return fphone_check  # If valid, return fPhone value\n",
    "\n",
    "# Apply the validation function\n",
    "client_phone_1['ValidatedPhone'] = client_phone_1.apply(validate_phone, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ff9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_phone_1 = client_phone_1 [['accountId','ValidatedPhone']]\n",
    "client_phone_1 = client_phone_1.rename(columns = {'ValidatedPhone' : 'Consider'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82bdfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_manual = client_phone_1.copy()\n",
    "client_manual['Consider'].replace('', np.nan, inplace=True)\n",
    "client_manual.drop_duplicates(subset=['accountId'], inplace=True)\n",
    "client_manual['Consider'].fillna('no phone found', inplace=True)\n",
    "client_manual['accountId'] = client_manual['accountId'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674eb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dials_merged = unique_calls.merge(client_manual, on='Consider', how='left')\n",
    "Deposits_merged = filtered_cdb.merge(client_manual, on='accountId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Dials_merged.merge(Deposits_merged[['accountId','deposit_amount']], on='accountId', how='left')\n",
    "final = a.groupby(['CRM ID', 'Date']).agg({\n",
    "    'accountId': 'nunique',   # counts unique accountId\n",
    "    'deposit_amount': 'sum'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed13368",
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = formatted_df.merge(final, how='left', left_on=['CRM ID','Date'], right_on=['CRM ID','Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc929e",
   "metadata": {},
   "source": [
    "## Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bccbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r'C:\\Users\\Akhil\\Downloads\\project\\TradeX_report'\n",
    "df.to_csv(f'{save_path}\\\\Summary_29_09.csv', index=False)\n",
    "D.to_csv(f'{save_path}\\\\Dialer_29_09.csv', index=False)\n",
    "crm_id_null_df.to_csv(f'{save_path}\\\\Not_Found_Users_29_09.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e81dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597db65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e6b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
